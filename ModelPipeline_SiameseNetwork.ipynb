{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building and Training Siamese Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local package imports\n",
    "from src.data_loader import DataSplitter\n",
    "from src.data_loader.FaceRecognitionDataset import FaceRecognitionDataset\n",
    "from src.model.FaceNet import SiameseNetwork\n",
    "from src.trainer.FaceNetTrainer import SiameseNetworkTrainer\n",
    "from src.utils import mytensorboard\n",
    "import src.utils.utils_images as img_util\n",
    "\n",
    "# Pytorch imports \n",
    "import torch\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "# General imports\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>General/Global Variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "dataset_dir =\"./src/data/celeba_dataset/images/\"\n",
    "labels_path=\"./src/data/celeba_dataset/labels.txt\"\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "val_ratio = 0.5\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data loading and preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Create dataset...\n",
      "Created dataset! Number of samples: 70150\n",
      "--------------------------------------------------------------------------\n",
      "Split dataset...\n",
      "Splitted dataset! Number of training samples: 35075, number of validation samples: 35075\n",
      "--------------------------------------------------------------------------\n",
      "Create data loaders...\n",
      "Created data loaders! Batch size: 256\n",
      "Number of batches in train_loader: 138\n",
      "Number of batches in val_loader: 138\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(\"Create dataset...\")\n",
    "dataset = FaceRecognitionDataset(dataset_dir=dataset_dir, labels_path=labels_path)\n",
    "print(f\"Created dataset! Number of samples: {len(dataset)}\")\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(\"Split dataset...\")\n",
    "train_dataset, val_dataset = DataSplitter.split_train_test(dataset=dataset, val_ratio=val_ratio)\n",
    "print(f\"Splitted dataset! Number of training samples: {len(train_dataset)}, number of validation samples: {len(val_dataset)}\")\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(\"Create data loaders...\")\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           num_workers=4,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=4)\n",
    "print(f\"Created data loaders! Batch size: {batch_size}\")\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in val_loader: {len(val_loader)}\")\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model building and training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Create model...\n",
      "Created model!\n",
      "Model is on cuda: True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1       [256, 128, 510, 510]           3,584\n",
      "         MaxPool2d-2       [256, 128, 255, 255]               0\n",
      "             PReLU-3       [256, 128, 255, 255]             128\n",
      "            Conv2d-4        [256, 64, 253, 253]          73,792\n",
      "         MaxPool2d-5        [256, 64, 126, 126]               0\n",
      "             PReLU-6        [256, 64, 126, 126]              64\n",
      "            Conv2d-7        [256, 32, 124, 124]          18,464\n",
      "         MaxPool2d-8          [256, 32, 62, 62]               0\n",
      "             PReLU-9          [256, 32, 62, 62]              32\n",
      "           Conv2d-10          [256, 16, 60, 60]           4,624\n",
      "        MaxPool2d-11          [256, 16, 30, 30]               0\n",
      "            PReLU-12          [256, 16, 30, 30]              16\n",
      "           Conv2d-13           [256, 8, 28, 28]           1,160\n",
      "        MaxPool2d-14           [256, 8, 14, 14]               0\n",
      "            PReLU-15           [256, 8, 14, 14]               8\n",
      "           Linear-16                 [256, 512]         803,328\n",
      "            PReLU-17                 [256, 512]               1\n",
      "           Linear-18                 [256, 256]         131,328\n",
      "           Conv2d-19       [256, 128, 510, 510]           3,584\n",
      "        MaxPool2d-20       [256, 128, 255, 255]               0\n",
      "            PReLU-21       [256, 128, 255, 255]             128\n",
      "           Conv2d-22        [256, 64, 253, 253]          73,792\n",
      "        MaxPool2d-23        [256, 64, 126, 126]               0\n",
      "            PReLU-24        [256, 64, 126, 126]              64\n",
      "           Conv2d-25        [256, 32, 124, 124]          18,464\n",
      "        MaxPool2d-26          [256, 32, 62, 62]               0\n",
      "            PReLU-27          [256, 32, 62, 62]              32\n",
      "           Conv2d-28          [256, 16, 60, 60]           4,624\n",
      "        MaxPool2d-29          [256, 16, 30, 30]               0\n",
      "            PReLU-30          [256, 16, 30, 30]              16\n",
      "           Conv2d-31           [256, 8, 28, 28]           1,160\n",
      "        MaxPool2d-32           [256, 8, 14, 14]               0\n",
      "            PReLU-33           [256, 8, 14, 14]               8\n",
      "           Linear-34                 [256, 512]         803,328\n",
      "            PReLU-35                 [256, 512]               1\n",
      "           Linear-36                 [256, 256]         131,328\n",
      "           Conv2d-37       [256, 128, 510, 510]           3,584\n",
      "        MaxPool2d-38       [256, 128, 255, 255]               0\n",
      "            PReLU-39       [256, 128, 255, 255]             128\n",
      "           Conv2d-40        [256, 64, 253, 253]          73,792\n",
      "        MaxPool2d-41        [256, 64, 126, 126]               0\n",
      "            PReLU-42        [256, 64, 126, 126]              64\n",
      "           Conv2d-43        [256, 32, 124, 124]          18,464\n",
      "        MaxPool2d-44          [256, 32, 62, 62]               0\n",
      "            PReLU-45          [256, 32, 62, 62]              32\n",
      "           Conv2d-46          [256, 16, 60, 60]           4,624\n",
      "        MaxPool2d-47          [256, 16, 30, 30]               0\n",
      "            PReLU-48          [256, 16, 30, 30]              16\n",
      "           Conv2d-49           [256, 8, 28, 28]           1,160\n",
      "        MaxPool2d-50           [256, 8, 14, 14]               0\n",
      "            PReLU-51           [256, 8, 14, 14]               8\n",
      "           Linear-52                 [256, 512]         803,328\n",
      "            PReLU-53                 [256, 512]               1\n",
      "           Linear-54                 [256, 256]         131,328\n",
      "================================================================\n",
      "Total params: 3,109,587\n",
      "Trainable params: 3,109,587\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 5736.25\n",
      "Params size (MB): 11.86\n",
      "Estimated Total Size (MB): 5748.11\n",
      "----------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samii\\.conda\\envs\\applied-ml\\lib\\site-packages\\torchsummary\\torchsummary.py:93: RuntimeWarning: overflow encountered in long_scalars\n",
      "  total_output += np.prod(summary[layer][\"output_shape\"])\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(\"Create model...\")\n",
    "model = SiameseNetwork().to(device)\n",
    "print(\"Created model!\")\n",
    "print(f'Model is on cuda: {next(model.parameters()).is_cuda}')\n",
    "#print(model)\n",
    "summary(model, [(3, 512, 512),(3, 512, 512),(3, 512, 512)], batch_size=batch_size)\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Create tensorboard_writer...\n",
      "nonempty logpath\n",
      "logpath:  ..\\logs\\SiameseNetwork\\run_1(2)\n",
      "tensorboard is up: http://localhost:6006\n",
      "Created tensorboard_writer!\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(\"Create tensorboard_writer...\")\n",
    "tensorboard_writer = mytensorboard.MySummaryWriter(numb_batches=len(train_loader), batch_size=batch_size, experiment_name=\"SiameseNetwork\")\n",
    "print(\"Created tensorboard_writer!\")\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Create trainer...\n",
      "Created trainer!\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------------\")\n",
    "print(\"Create trainer...\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = SiameseNetworkTrainer(model=model, \n",
    "                                train_loader=train_loader,\n",
    "                                valid_loader=val_loader, \n",
    "                                test_loader=val_loader,\n",
    "                                optimizer=optimizer, \n",
    "                                tensorboard_writer=tensorboard_writer)\n",
    "print(\"Created trainer!\")\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tensorboard</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# anchors = images[0]\n",
    "# positives = images[1]\n",
    "# negatives = images[2]\n",
    "\n",
    "# # write graph of model to tensorboard\n",
    "# tensorboard_writer.add_graph(model, images)\n",
    "\n",
    "\n",
    "# # write sample images to tensorboard\n",
    "\n",
    "\n",
    "# anchors_grid = torchvision.utils.make_grid(anchors, nrow=batch_size)\n",
    "# tensorboard_writer.add_image(\"anchor sample\", anchors_grid)\n",
    "\n",
    "# positives_grid = torchvision.utils.make_grid(positives, nrow=batch_size)\n",
    "# tensorboard_writer.add_image(\"positives sample\", positives_grid)\n",
    "\n",
    "# negatives_grid = torchvision.utils.make_grid(negatives, nrow=batch_size)\n",
    "# tensorboard_writer.add_image(\"negatives sample\", negatives_grid)\n",
    "\n",
    "# total_grid = torchvision.utils.make_grid([anchors_grid, positives_grid, negatives_grid], nrow=1)\n",
    "# tensorboard_writer.add_image(\"sample\", total_grid)\n",
    "\n",
    "# fig = img_util.plot_classes_preds_face_recognition(anchors, labels, [\"1234\", \"1234\", \"1234\", \"1234\"])\n",
    "# tensorboard_writer.add_figure(\"predictions vs. actuals\", fig)\n",
    "\n",
    "# time.sleep(13)\n",
    "\n",
    "# # Deleting image variables to free RAM\n",
    "# anchors_grid = None\n",
    "# positives_grid = None\n",
    "# negatives_grid = None\n",
    "# total_grid = None\n",
    "# fig = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Training</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    }
   ],
   "source": [
    "print(\"start training\")\n",
    "\n",
    "trainer.train(epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-ml",
   "language": "python",
   "name": "applied-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
